# FR-P0-4: Explainability

**Status:** ‚úÖ Zaimplementowane (v1.0)

Pe≈Çna transparentno≈õƒá systemu RAG ‚Äî **widzisz dok≈Çadnie** kt√≥re fragmenty wesz≈Çy do kontekstu i **dlaczego**.

---

## Problem

Systemy RAG sƒÖ czƒôsto "czarnƒÖ skrzynkƒÖ":

```
Pytanie: "Kiedy jest deadline projektu?"

‚ùì Odpowied≈∫: "Deadline to 31 marca 2025"

Ale skƒÖd to wiem?
- Kt√≥re dokumenty zosta≈Çy u≈ºyte?
- Dlaczego w≈Ça≈õnie te?
- Ile by≈Ço kandydat√≥w?
- Jak dzia≈Ça≈Ç scoring?
```

**Bez wyja≈õnie≈Ñ:** brak zaufania do odpowiedzi, trudno≈õƒá w debugowaniu.

---

## RozwiƒÖzanie

Digital Twin oferuje **kompletne wyja≈õnienie** ka≈ºdego zapytania:

```python
result = engine.query(
    "Kiedy jest deadline projektu?",
    include_explanation=True
)

explanation = result["explanation"]
# Zawiera: dokumenty, scoring, timing, kontekst...
```

---

## Struktura wyja≈õnienia

### RAGExplanation (g≈Ç√≥wna struktura)

```python
@dataclass
class RAGExplanation:
    # Zapytanie
    query_text: str                    # Tekst zapytania
    query_embedding_model: str         # Model embedding√≥w

    # Retrieval
    retrieval_mode: str                # "similarity" lub "priority_weighted"
    retrieval_top_k: int               # Ile dokument√≥w pobrano
    documents_retrieved: list[RetrievalExplanation]

    # Kontekst
    context_window: ContextWindowExplanation

    # Generowanie
    response_mode: str                 # "compact", "refine", etc.
    llm_provider: str                  # "gpt4all", "openai", etc.
    llm_model: str                     # Nazwa modelu

    # Timing (milisekundy)
    retrieval_time_ms: float
    generation_time_ms: float
    total_time_ms: float

    # Filtry
    filters_applied: dict              # Aktywne filtry
    timestamp: datetime                # Czas zapytania
```

### RetrievalExplanation (per dokument)

```python
@dataclass
class RetrievalExplanation:
    # Identyfikacja
    document_id: str
    filename: str
    source_type: str

    # Scoring
    similarity_score: float      # Podobie≈Ñstwo wektorowe (0-1)
    priority_score: float        # Priorytet dokumentu (0-1)
    final_score: float           # Wa≈ºony wynik ko≈Ñcowy (0-1)

    # Rozbicie priorytetu
    type_contribution: float     # Wk≈Çad z typu dokumentu
    recency_contribution: float  # Wk≈Çad z aktualno≈õci
    approval_contribution: float # Wk≈Çad ze statusu zatwierdzenia

    # Pozycja
    rank: int                    # Miejsce w rankingu
    passed_filters: list[str]   # Kt√≥re filtry przeszed≈Ç
```

### ContextWindowExplanation (kontekst LLM)

```python
@dataclass
class ContextWindowExplanation:
    # Tokeny
    total_tokens: int            # U≈ºyte tokeny
    max_tokens: int              # Maksimum dozwolone
    utilization: float           # Procent wykorzystania

    # Fragmenty
    fragments: list[ContextFragment]
    fragment_count: int

    # Overflow
    overflow_documents: int      # Dokumenty pominiƒôte (brak miejsca)
    overflow_tokens: int         # Tokeny pominiƒôte
```

---

## U≈ºycie w kodzie

### Podstawowe wyja≈õnienie

```python
from src.rag import RAGEngine

engine = RAGEngine()

result = engine.query(
    "Jakie zadania mam na ten tydzie≈Ñ?",
    include_explanation=True
)

exp = result["explanation"]

# Podsumowanie
print(f"‚è±Ô∏è Czas: {exp['timing']['total_ms']:.0f}ms")
print(f"  - Retrieval: {exp['timing']['retrieval_ms']:.0f}ms")
print(f"  - Generation: {exp['timing']['generation_ms']:.0f}ms")
```

### Analiza pobranych dokument√≥w

```python
print(f"\nüìö Dokumenty ({len(exp['documents_retrieved'])}):")

for doc in exp["documents_retrieved"]:
    print(f"\n  {doc['rank']}. {doc['filename']}")
    print(f"     Typ: {doc['source_type']}")
    print(f"     Similarity: {doc['similarity_score']:.3f}")
    print(f"     Priority: {doc['priority_score']:.3f}")
    print(f"     Final: {doc['final_score']:.3f}")

    # Rozbicie priorytetu
    print(f"     Sk≈Çadniki:")
    print(f"       - Type: {doc['type_contribution']:.2f}")
    print(f"       - Recency: {doc['recency_contribution']:.2f}")
    print(f"       - Approval: {doc['approval_contribution']:.2f}")
```

### Analiza kontekstu

```python
ctx = exp["context_window"]

print(f"\nüìù Kontekst LLM:")
print(f"   Tokeny: {ctx['total_tokens']}/{ctx['max_tokens']} ({ctx['utilization']:.0%})")
print(f"   Fragment√≥w: {ctx['fragment_count']}")

if ctx["overflow_documents"] > 0:
    print(f"   ‚ö†Ô∏è Pominiƒôto {ctx['overflow_documents']} dokument√≥w (brak miejsca)")

print(f"\n   Fragmenty:")
for frag in ctx["fragments"]:
    print(f"   - [{frag['source_type']}] {frag['source_id']}")
    print(f"     {frag['text'][:60]}...")
    print(f"     ({frag['token_count']} token√≥w)")
```

### Pe≈Çny przyk≈Çad

```python
def explain_query(question: str):
    """Wykonaj zapytanie i wy≈õwietl pe≈Çne wyja≈õnienie."""
    engine = RAGEngine()
    result = engine.query(question, include_explanation=True)

    print(f"‚ùì Pytanie: {question}\n")
    print(f"üí¨ Odpowied≈∫: {result['answer']}\n")
    print("=" * 60)

    exp = result["explanation"]

    # Timing
    print(f"\n‚è±Ô∏è TIMING")
    print(f"   Total: {exp['timing']['total_ms']:.0f}ms")
    print(f"   Retrieval: {exp['timing']['retrieval_ms']:.0f}ms")
    print(f"   Generation: {exp['timing']['generation_ms']:.0f}ms")

    # Retrieval
    print(f"\nüîç RETRIEVAL")
    print(f"   Mode: {exp['retrieval_mode']}")
    print(f"   Top-K: {exp['retrieval_top_k']}")
    print(f"   Embedding: {exp['query_embedding_model']}")

    # Dokumenty
    print(f"\nüìö DOKUMENTY POBRANE ({len(exp['documents_retrieved'])})")
    for doc in exp["documents_retrieved"]:
        print(f"\n   [{doc['rank']}] {doc['filename']}")
        print(f"       sim={doc['similarity_score']:.2f} "
              f"pri={doc['priority_score']:.2f} "
              f"‚Üí {doc['final_score']:.2f}")

    # Kontekst
    ctx = exp["context_window"]
    print(f"\nüìù KONTEKST")
    print(f"   Tokens: {ctx['total_tokens']}/{ctx['max_tokens']} ({ctx['utilization']:.0%})")
    print(f"   Fragments: {ctx['fragment_count']}")

    # LLM
    print(f"\nü§ñ LLM")
    print(f"   Provider: {exp['llm_provider']}")
    print(f"   Model: {exp['llm_model']}")
    print(f"   Mode: {exp['response_mode']}")

# U≈ºycie
explain_query("Kiedy jest deadline projektu Alpha?")
```

---

## Przyk≈Çadowy output

```
‚ùì Pytanie: Kiedy jest deadline projektu Alpha?

üí¨ Odpowied≈∫: Deadline projektu Alpha to 31 marca 2025, zgodnie z
ostatnimi ustaleniami z zespo≈Çem.
[≈πr√≥d≈Ço: email, 2024-12-10, "Deadline Alpha: 31.03.2025"]

============================================================

‚è±Ô∏è TIMING
   Total: 1247ms
   Retrieval: 89ms
   Generation: 1158ms

üîç RETRIEVAL
   Mode: priority_weighted
   Top-K: 5
   Embedding: sentence-transformers/all-MiniLM-L6-v2

üìö DOKUMENTY POBRANE (5)

   [1] email_2024-12-10_deadline.eml
       sim=0.89 pri=0.72 ‚Üí 0.84

   [2] notes/projekt-alpha.md
       sim=0.85 pri=0.65 ‚Üí 0.79

   [3] whatsapp_team_alpha.txt
       sim=0.91 pri=0.35 ‚Üí 0.74

   [4] email_2024-11-15_planning.eml
       sim=0.78 pri=0.58 ‚Üí 0.72

   [5] notes/spotkanie-2024-11.md
       sim=0.75 pri=0.55 ‚Üí 0.69

üìù KONTEKST
   Tokens: 1847/4000 (46%)
   Fragments: 5

ü§ñ LLM
   Provider: gpt4all
   Model: mistral-7b-instruct-v0.1.Q4_0.gguf
   Mode: compact
```

---

## Formatowanie dla u≈ºytkownika

### Czytelne podsumowanie

```python
from src.rag.explainability import format_explanation_summary

result = engine.query("...", include_explanation=True)

# Sformatowane podsumowanie
summary = format_explanation_summary(result["explanation"])
print(summary)
```

**Output:**
```
Query processed in 1247ms
  Retrieval: 89ms (priority_weighted)
  Generation: 1158ms (gpt4all)

Documents retrieved: 5
  Top sources:
    1. email_2024-12-10_deadline.eml (sim: 0.89, pri: 0.72)
    2. notes/projekt-alpha.md (sim: 0.85, pri: 0.65)
    3. whatsapp_team_alpha.txt (sim: 0.91, pri: 0.35)

Context: 1847/4000 tokens (46% used)
```

---

## UI ‚Äî wy≈õwietlanie wyja≈õnie≈Ñ

W Streamlit mo≈ºna stworzyƒá zak≈Çadki:

```python
import streamlit as st

def render_explanation(explanation: dict):
    """Renderuj wyja≈õnienie w Streamlit."""

    tabs = st.tabs(["üìö Dokumenty", "üìù Kontekst", "‚è±Ô∏è Timing", "üîß Szczeg√≥≈Çy"])

    with tabs[0]:  # Dokumenty
        for doc in explanation["documents_retrieved"]:
            with st.expander(f"{doc['rank']}. {doc['filename']}"):
                col1, col2, col3 = st.columns(3)
                col1.metric("Similarity", f"{doc['similarity_score']:.2f}")
                col2.metric("Priority", f"{doc['priority_score']:.2f}")
                col3.metric("Final", f"{doc['final_score']:.2f}")

                st.write(f"**Typ:** {doc['source_type']}")

    with tabs[1]:  # Kontekst
        ctx = explanation["context_window"]
        st.progress(ctx["utilization"])
        st.write(f"**Tokeny:** {ctx['total_tokens']}/{ctx['max_tokens']}")
        st.write(f"**Fragment√≥w:** {ctx['fragment_count']}")

    with tabs[2]:  # Timing
        timing = explanation["timing"]
        st.metric("Total", f"{timing['total_ms']:.0f}ms")
        st.metric("Retrieval", f"{timing['retrieval_ms']:.0f}ms")
        st.metric("Generation", f"{timing['generation_ms']:.0f}ms")

    with tabs[3]:  # Szczeg√≥≈Çy
        st.json(explanation)
```

---

## Debugging z wyja≈õnieniami

### Dlaczego nie znaleziono dokumentu?

```python
# Sprawd≈∫ parametry retrievalu
print(f"Top-K: {exp['retrieval_top_k']}")
print(f"Mode: {exp['retrieval_mode']}")

# Sprawd≈∫ scoring pierwszego dokumentu
top_doc = exp["documents_retrieved"][0]
print(f"Najlepszy similarity: {top_doc['similarity_score']}")
# Je≈õli niski (<0.5) ‚Äî zapytanie nie pasuje semantycznie

# Sprawd≈∫ czy priorytet nie zdominowa≈Ç
print(f"Priority weight: {settings.priority_document_weight}")
```

### Dlaczego odpowied≈∫ jest wolna?

```python
timing = exp["timing"]

if timing["retrieval_ms"] > 500:
    print("‚ö†Ô∏è Wolny retrieval ‚Äî sprawd≈∫ indeks Qdrant")

if timing["generation_ms"] > 5000:
    print("‚ö†Ô∏è Wolna generacja ‚Äî rozwa≈º mniejszy model lub TOP_K")
```

### Dlaczego kontekst jest prze≈Çadowany?

```python
ctx = exp["context_window"]

if ctx["utilization"] > 0.9:
    print("‚ö†Ô∏è Kontekst prawie pe≈Çny")
    print(f"   Overflow: {ctx['overflow_documents']} dokument√≥w")
    print("   RozwiƒÖzanie: zmniejsz TOP_K lub CHUNK_SIZE")
```

---

## Konfiguracja

### W≈ÇƒÖczanie wyja≈õnie≈Ñ

```python
# Per-zapytanie
result = engine.query("...", include_explanation=True)

# Domy≈õlnie wy≈ÇƒÖczone (oszczƒôdno≈õƒá token√≥w)
result = engine.query("...")  # Bez explanation
```

### Maksymalna ilo≈õƒá kontekstu

```bash
# .env ‚Äî dla kontekstu LLM
CHUNK_SIZE=512
TOP_K=5
```

---

## PowiƒÖzane

- **[FR-P0-3: Priority Rules](FR-P0-3-Priority-Rules)** ‚Äî jak dzia≈Ça scoring
- **[FR-P0-1: Grounded Answers](FR-P0-1-Grounded-Answers)** ‚Äî cytaty w odpowiedziach
- **[Pipelines](Pipelines)** ‚Äî przep≈Çyw danych przez RAG

---

<p align="center">
  <a href="FR-P0-3-Priority-Rules">‚Üê FR-P0-3: Priority Rules</a> |
  <a href="Home">Strona g≈Ç√≥wna</a> |
  <a href="FR-P0-5-Forget-RTBF">FR-P0-5: Forget/RTBF ‚Üí</a>
</p>
